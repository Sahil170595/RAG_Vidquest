### ğŸ—‚ï¸ `README.md` 


# ğŸ“ AI Video Chat Assistant

A Retrieval-Augmented Generation (RAG) system for querying course lecture videos. Users can ask natural language questions and receive relevant video clips, transcripts, and summaries grounded directly in the lecture content.

# Architecture

* ![image](https://github.com/user-attachments/assets/54865242-10b5-4448-a710-3e42a867d370)

---

## ğŸš€ Demo

Launch the Gradio app and try queries like:

â€œUsing only the videos, explain how ResNets work.â€
â€œUsing only the videos, explain the advantages of CNNs over fully connected networks.â€
â€œUsing only the videos, explain the the binary cross entropy loss function.â€
---

## ğŸ§± Project Architecture

- **ETL Pipeline**: Extracts frames and aligns them with subtitles.
- **Featurization**: Embeds merged sentences with `SentenceTransformer`.
- **Vector Search**: Qdrant for nearest neighbor lookup.
- **Summarization**: Uses `Ollama` (Gemma 3) to summarize the top transcript matches.
- **Frontend**: Gradio app for interactive querying and video playback.

---

## ğŸ“ Directory Structure

```

video-chat-assistant/
â”œâ”€â”€ app/                  # Gradio UI and logic
â”œâ”€â”€ data/                 # Small sample inputs
â”œâ”€â”€ etl/                  # Subtitle/frame extraction scripts
â”œâ”€â”€ vector\_db/            # Qdrant upload scripts
â”œâ”€â”€ notebooks/            # Exploration & testing
â”œâ”€â”€ clips\_output/         # Output clips (small samples only)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## ğŸ“¦ Full Dataset Access

Due to GitHub file limits, full video data is **not included**. To test on complete data:

1. Download from https://huggingface.co/datasets/aegean-ai/ai-lectures-spring-24

---


Make sure MongoDB and Qdrant are running, then run:

```bash
python app/main.py  # or open the notebook to launch
```

---

## ğŸ“Š Example Outputs

| Query              | Summary                 | Clip |
| ------------------ | ----------------------- | ---- |
| What is padding?   | A padded convolution... | âœ…    |
| ResNet motivation? | Skipping connections... | âœ…    |

---

```markdown
# ğŸ“¦ Dataset Setup Guide

This project uses a collection of lecture videos and their subtitles in `.vtt` format.

---

## Option 1: Manual Download

Download the raw dataset (videos + subtitles) here:

ğŸ”— [Google Drive - Video Dataset](https://your-drive-link)

Place them in the following structure:
````

data/
â””â”€â”€ videos/
â”œâ”€â”€ xyz123.mp4
â”œâ”€â”€ xyz123.vtt
â””â”€â”€ ...

````

---

## Option 2: Rebuild from Raw Files

Run the subtitle and frame extraction:

```bash
python etl/extract_subtitles.py
python etl/extract_frames.py
````

MongoDB must be running and populated with aligned subtitle data for this to work.

---

## Output Samples

* `output/`: contains `.mp4` slices of relevant segments
* ![image](https://github.com/user-attachments/assets/df5d3601-fa88-4410-9e53-14b3d3eb4fe6)


